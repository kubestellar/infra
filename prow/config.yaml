prowjob_namespace: prow
pod_namespace: test-pods
job_config_path: prow/jobs

plank:
  pod_pending_timeout: 60m
  pod_unscheduled_timeout: 60m

  report_templates:
    "*": "[Full PR test history](https://prow2-private.kubestellar.io/pr-history?org={{.Spec.Refs.Org}}&repo={{.Spec.Refs.Repo}}&pr={{with index .Spec.Refs.Pulls 0}}{{.Number}}{{end}})"
    "kubestellar/kubestellar": "[Full PR test history](https://prow2.kubestellar.io/pr-history?org={{.Spec.Refs.Org}}&repo={{.Spec.Refs.Repo}}&pr={{with index .Spec.Refs.Pulls 0}}{{.Number}}{{end}})"
    "kubestellar/ui": "[Full PR test history](https://prow2.kubestellar.io/pr-history?org={{.Spec.Refs.Org}}&repo={{.Spec.Refs.Repo}}&pr={{with index .Spec.Refs.Pulls 0}}{{.Number}}{{end}})"
    "kubestellar/kubeflex": "[Full PR test history](https://prow2.kubestellar.io/pr-history?org={{.Spec.Refs.Org}}&repo={{.Spec.Refs.Repo}}&pr={{with index .Spec.Refs.Pulls 0}}{{.Number}}{{end}})"
    "kubestellar/a2a": "[Full PR test history](https://prow2.kubestellar.io/pr-history?org={{.Spec.Refs.Org}}&repo={{.Spec.Refs.Repo}}&pr={{with index .Spec.Refs.Pulls 0}}{{.Number}}{{end}})"
    "kubestellar/kubectl-plugin": "[Full PR test history](https://prow2.kubestellar.io/pr-history?org={{.Spec.Refs.Org}}&repo={{.Spec.Refs.Repo}}&pr={{with index .Spec.Refs.Pulls 0}}{{.Number}}{{end}})"
    "kubestellar/docs": "[Full PR test history](https://prow2.kubestellar.io/pr-history?org={{.Spec.Refs.Org}}&repo={{.Spec.Refs.Repo}}&pr={{with index .Spec.Refs.Pulls 0}}{{.Number}}{{end}})"
    "kubestellar/infra": "[Full PR test history](https://prow2.kubestellar.io/pr-history?org={{.Spec.Refs.Org}}&repo={{.Spec.Refs.Repo}}&pr={{with index .Spec.Refs.Pulls 0}}{{.Number}}{{end}})"

  job_url_prefix_config:
    "*": https://prow2-private.kubestellar.io/view/
    "kubestellar/kubestellar": "https://prow2.kubestellar.io/view/"
    "kubestellar/ui": "https://prow2.kubestellar.io/view/"
    "kubestellar/kubeflex": "https://prow2.kubestellar.io/view/"
    "kubestellar/a2a": "https://prow2.kubestellar.io/view/"
    "kubestellar/kubectl-plugin": "https://prow2.kubestellar.io/view/"
    "kubestellar/docs": "https://prow2.kubestellar.io/view/"
    "kubestellar/infra": "https://prow2.kubestellar.io/view/"

  default_decoration_config_entries:
    - config:
        timeout: 2h
        grace_period: 5m
        utility_images:
          clonerefs: us-docker.pkg.dev/k8s-infra-prow/images/clonerefs:v20240802-66b115076
          entrypoint: us-docker.pkg.dev/k8s-infra-prow/images/entrypoint:v20240802-66b115076
          initupload: us-docker.pkg.dev/k8s-infra-prow/images/initupload:v20240802-66b115076
          sidecar: us-docker.pkg.dev/k8s-infra-prow/images/sidecar:v20240802-66b115076
        gcs_configuration:
          bucket: "s3://prow-logs"
          path_strategy: "explicit"
        s3_credentials_secret: "s3-credentials"
        github_api_endpoints:
          - http://ghproxy
          - https://api.github.com
        github_app_id: "1227751"
        github_app_private_key_secret:
          name: github-token
          key: cert

in_repo_config:
  enabled:
    "*": true

sinker:
  # How often sinker resyncs, defaults to 1h
  resync_period: 1m
  # How long build pods are kept after finishing, defaults to 24h
  max_pod_age: 2h
  # How long to keep ProwJob CRs - reduced from 28 days to 7 days
  # to improve PR dashboard loading performance
  max_prowjob_age: 168h

deck:
  spyglass:
    gcs_browser_prefix: "https://gcsweb.kubestellar.io/prow-logs/"
    lenses:
      - required_files:
          - ^(?:started|finished)\.json$
        optional_files:
          - ^(?:podinfo|prowjob)\.json$
        lens:
          name: metadata
      - required_files:
          - build-log.txt
        lens:
          name: buildlog
          config:
            highlight_regexes:
              - timed out
              - "ERROR"
              - (FAIL|Failure \[)\b
              - panic\b
              - ^E\d{4} \d\d:\d\d:\d\d\.\d\d\d]
              - unbound variable
              - no more retries left\.
              - ^Retry [0-9]+\/[0-9]+ exited [^0], retrying in [0-9]+ seconds
              - npm ERR!
              - "WARNING: DATA RACE"
      - required_files:
          - artifacts/junit.*\.xml
        lens:
          name: junit
    size_limit: 500000000 # 500MB

github_reporter:
  job_types_to_report:
    - presubmit
    - postsubmit

tide:
  pr_status_base_urls:
    "*": https://prow2-private.kubestellar.io/
  merge_method:
    kubestellar: merge
    kubestellar/kubestellar: merge
    kubestellar/ui: squash
    kubestellar/kubeflex: merge
    kubestellar/infra: merge
    kubestellar/a2a: squash
    kubestellar/kubectl-plugin: squash
    kubestellar/docs: squash
  queries:
    # no release notes
    - repos:
        - kubestellar/kubestellar
        - kubestellar/kubeflex
        - kubestellar/infra
        - kubestellar/kubectl-plugin
      labels:
        - lgtm
        - approved
      missingLabels:
        - "dco-signoff: no"
        - do-not-merge
        - do-not-merge/hold
        - do-not-merge/invalid-owners-file
        - do-not-merge/work-in-progress
        - needs-rebase
    # Query for UI repo without DCO requirement
    - repos:
        - kubestellar/ui
      labels:
        - lgtm
        - approved
      missingLabels:
        - do-not-merge
        - do-not-merge/hold
        - do-not-merge/invalid-owners-file
        - do-not-merge/work-in-progress
        - needs-rebase
    # Query for A2A repo without DCO requirement
    - repos:
        - kubestellar/a2a
      labels:
        - lgtm
        - approved
      missingLabels:
        - do-not-merge
        - do-not-merge/hold
        - do-not-merge/invalid-owners-file
        - do-not-merge/work-in-progress
        - needs-rebase
    # Query for docs repository without DCO requirement
    - repos:
        - kubestellar/docs
      labels:
        - lgtm
        - approved
      missingLabels:
        - do-not-merge
        - do-not-merge/hold
        - do-not-merge/invalid-owners-file
        - do-not-merge/work-in-progress
        - needs-rebase

branch-protection:
  orgs:
    kubestellar:
      repos:
        kubestellar:
          protect: true
          required_status_checks:
            contexts:
              - dco
          restrictions:
            users: []
            teams:
              - kubestellar/kubestellar-admins
          include:
            - "^main$"
            - "^release-.+$"
        ui:
          protect: true
          required_status_checks:
            contexts: []
          restrictions:
            users: []
            teams:
              - kubestellar/kubestellar-ui-admins
          include:
            - "^main$"
            - "^dev$"
            - "^release-.+$"
        kubeflex:  # Added KubeFlex with DCO check
          protect: true
          required_status_checks:
            contexts:
              - dco
          restrictions:
            users: []
            teams:
              - kubestellar/kubeflex-admins
          include:
            - "^main$"
            - "^release-.+$"
        kubectl-plugin:  # Added kubectl-plugin with DCO check
          protect: true
          required_status_checks:
            contexts:
              - dco
          restrictions:
            users: []
            teams:
              - kubestellar/kubestellar-admins
          include:
            - "^main$"
            - "^release-.+$"
        infra:
          protect: true
          required_status_checks:
            contexts:
              - dco
          restrictions:
            users: []
            teams:
              - kubestellar/kubestellar-admins
          include:
            - "^main$"
            - "^release-.+$"
        a2a:  # Added A2A without DCO check
          protect: true
          required_status_checks:
            contexts: []
          restrictions:
            users: []
            teams:
              - kubestellar/kubestellar-a2a-maintainers
          include:
            - "^main$"
            - "^release-.+$"
        docs:  # Added docs repository without DCO check
          protect: true
          required_status_checks:
            contexts: []
          restrictions:
            users: []
            teams:
              - kubestellar/kubestellar-docs-maintainers
              - kubestellar/kubestellar-ui-admins
          include:
            - "^main$"
            - "^dev$"
            - "^release-.+$"

# decorate_all_jobs: true

presets:

  ################################################################
  # (prow-cluster only) username and password for GHCR

  - labels:
      preset-ghcr-credentials: "true"
    env:
      - name: KUBESTELLAR_GHCR_USERNAME
        valueFrom:
          secretKeyRef:
            name: ghcr-credentials
            key: username
      - name: KUBESTELLAR_GHCR_PASSWORD
        valueFrom:
          secretKeyRef:
            name: ghcr-credentials
            key: password

periodics:
  - interval: 60m  # Existing echo-test job
    agent: kubernetes
    name: echo-test
    spec:
      containers:
        - image: ghcr.io/kubestellar/infra/alpine:3.22.0
          command: ["/bin/date"]

  # - interval: 60m  # New periodic job
  #   agent: kubernetes
  #   name: config-bootstrapper-job  # Name for the new job
  #   decorate: true
  #   clone_uri: "https://github.com/kubestellar/infra.git"
  #   spec:
  #     containers:
  #       - image: gcr.io/k8s-prow/config-bootstrapper:latest
  #         name: config-bootstrapper
  #         command:
  #           - /app/config-bootstrapper
  #         args:
  #           - --dry-run=false
  #           - --config-path=/home/prow/go/src/github.com/kubestellar/kubestellar/prow/config.yaml
  #           - --plugin-config=/home/prow/go/src/github.com/kubestellar/kubestellar/prow/plugins.yaml
  #           - --label-config=/home/prow/go/src/github.com/kubestellar/kubestellar/prow/labels.yaml
  #           - --job-config-path=/home/prow/go/src/github.com/kubestellar/kubestellar/prow/jobs