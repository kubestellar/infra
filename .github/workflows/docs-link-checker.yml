name: Docs Link Checker

on:
  schedule:
    - cron: '0 6 * * 1'  # Every Monday at 6 AM UTC
  workflow_dispatch:

permissions:
  contents: read

jobs:
  check-links:
    runs-on: ubuntu-latest
    steps:
      - name: Check links on kubestellar.io
        id: check
        env:
          GH_TOKEN: ${{ secrets.WORKFLOW_SYNC_TOKEN }}
        run: |
          set -euo pipefail

          # Install lychee link checker
          curl -sLO https://github.com/lycheeverse/lychee/releases/download/v0.15.1/lychee-v0.15.1-x86_64-unknown-linux-gnu.tar.gz
          tar xzf lychee-v0.15.1-x86_64-unknown-linux-gnu.tar.gz
          chmod +x lychee

          # Create output directory
          mkdir -p results

          # Check kubestellar.io (main site)
          echo "Checking kubestellar.io..."
          ./lychee --no-progress --format json --output results/main.json \
            --exclude "github.com.*edit" \
            --exclude "linkedin.com" \
            --exclude "twitter.com" \
            --exclude "x.com" \
            --exclude "slack.com" \
            --exclude "forms.gle" \
            --exclude "localhost" \
            --exclude "127.0.0.1" \
            --max-redirects 5 \
            --timeout 30 \
            --retry-wait-time 5 \
            --max-retries 3 \
            "https://kubestellar.io" || true

          # Check docs site
          echo "Checking docs.kubestellar.io..."
          ./lychee --no-progress --format json --output results/docs.json \
            --exclude "github.com.*edit" \
            --exclude "linkedin.com" \
            --exclude "twitter.com" \
            --exclude "x.com" \
            --exclude "slack.com" \
            --exclude "forms.gle" \
            --exclude "localhost" \
            --exclude "127.0.0.1" \
            --max-redirects 5 \
            --timeout 30 \
            --retry-wait-time 5 \
            --max-retries 3 \
            "https://docs.kubestellar.io" || true

          # Combine and extract 404 errors
          echo "Processing results..."
          cat results/*.json 2>/dev/null | jq -s 'add' > results/combined.json || echo '{"fail_map":{}}' > results/combined.json

          # Extract failed URLs with status and source pages
          jq -r '.fail_map // {} | to_entries[] | "\(.key)|\(.value[0].status // "unknown")|\(.value[0].source // "unknown")"' results/combined.json > results/broken_links.txt || touch results/broken_links.txt

          # Create a list of broken URL hashes for comparison
          while IFS='|' read -r url status source; do
            [ -z "$url" ] && continue
            echo "$url" | md5sum | cut -c1-8
          done < results/broken_links.txt > results/broken_hashes.txt

          echo "Found broken links:"
          cat results/broken_links.txt || echo "None"

          # Count broken links
          broken_count=$(wc -l < results/broken_links.txt | tr -d ' ')
          echo "broken_count=$broken_count" >> $GITHUB_OUTPUT

      - name: Close fixed link issues
        env:
          GH_TOKEN: ${{ secrets.WORKFLOW_SYNC_TOKEN }}
        run: |
          set -euo pipefail

          echo "Checking for issues with fixed links..."

          # Get all open issues with "Broken link:" in the title
          gh issue list --repo kubestellar/docs \
            --search "Broken link: in:title" \
            --state open \
            --json number,title,body \
            --limit 100 > results/open_issues.json || echo "[]" > results/open_issues.json

          # Process each open issue
          jq -c '.[]' results/open_issues.json | while read -r issue; do
            number=$(echo "$issue" | jq -r '.number')
            title=$(echo "$issue" | jq -r '.title')
            body=$(echo "$issue" | jq -r '.body')

            # Extract the hash from the issue body
            issue_hash=$(echo "$body" | grep -oP '(?<=Hash:\*\* )[a-f0-9]+' || echo "")

            if [ -z "$issue_hash" ]; then
              echo "Could not find hash in issue #$number, skipping"
              continue
            fi

            # Check if this hash is still in our broken links
            if grep -q "^${issue_hash}$" results/broken_hashes.txt 2>/dev/null; then
              echo "Issue #$number: Link still broken (hash: $issue_hash)"
            else
              echo "Issue #$number: Link appears to be fixed, closing..."
              close_comment=$(printf '%s\n\n%s' \
                "This link appears to be fixed! Verified by the automated link checker." \
                "If this was closed in error, please reopen the issue.")
              gh issue close "$number" --repo kubestellar/docs \
                --comment "$close_comment" || echo "Failed to close issue #$number"
              sleep 1
            fi
          done

          echo "Done checking for fixed links"

      - name: Create issues for broken links
        if: steps.check.outputs.broken_count > 0
        env:
          GH_TOKEN: ${{ secrets.WORKFLOW_SYNC_TOKEN }}
        run: |
          set -euo pipefail

          # Check each broken link and create issue if not already exists
          while IFS='|' read -r url status source; do
            [ -z "$url" ] && continue

            # Create a unique identifier for the issue
            url_hash=$(echo "$url" | md5sum | cut -c1-8)
            issue_title="Broken link: $(echo "$url" | cut -c1-70)..."

            # Check if issue already exists (open)
            existing=$(gh issue list --repo kubestellar/docs \
              --search "Hash:** $url_hash in:body" \
              --state open \
              --json number \
              --jq '.[0].number' 2>/dev/null || echo "")

            if [ -n "$existing" ] && [ "$existing" != "null" ]; then
              echo "Issue already exists for $url (issue #$existing)"
              continue
            fi

            # Create new issue
            echo "Creating issue for broken link: $url (status: $status, source: $source)"
            issue_body=$(printf '## Broken Link Detected\n\n**Broken URL:** [%s](%s)\n**Status:** %s\n**Found on page:** [%s](%s)\n**Hash:** %s\n\n### How to fix\n1. Go to the source page listed above\n2. Find and update the broken link to a working URL, or remove it if no longer relevant\n3. Submit a PR with the fix\n\n---\nAuto-generated by [docs-link-checker](https://github.com/kubestellar/infra/actions/workflows/docs-link-checker.yml)' "$url" "$url" "$status" "$source" "$source" "$url_hash")
            gh issue create --repo kubestellar/docs \
              --title "$issue_title" \
              --label "help wanted,bug,documentation" \
              --body "$issue_body" || echo "Failed to create issue for $url"

            # Rate limit to avoid API issues
            sleep 2
          done < results/broken_links.txt

          echo "Done processing broken links"
